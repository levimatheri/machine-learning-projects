{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>TF Mutiple Hidden Layers: Regression on Boston Data\n",
    "<br />\n",
    "Batched, Parameterized, with Dropout</h1>\n",
    "<br />\n",
    "This is adapted from Frossard's <a href=\"http://www.cs.toronto.edu/~frossard/post/tensorflow/\">tutorial</a>.\n",
    "<br />\n",
    "This approach is not batched, and the number of layers is fixed.\n",
    "<br />\n",
    "<br />\n",
    "D. Thiebaut\n",
    "<br />August 2016\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import the Libraries and Tools</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn import cross_validation\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from __future__ import print_function\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Import the Boston Data</h2>\n",
    "<br />\n",
    "We don't worry about adding column names to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of Boston test_x =  (102, 13)\n",
      "Dimension of test_y =  (102, 1)\n",
      "Dimension of Boston train_x =  (404, 13)\n",
      "Dimension of train_y =  (404, 1)\n"
     ]
    }
   ],
   "source": [
    "boston = learn.datasets.load_dataset('boston')\n",
    "#print( \"boston = \", boston )\n",
    "x, y = boston.data, boston.target\n",
    "y.resize( y.size, 1 ) #make y = [[x], [x], [x], ... ]\n",
    "\n",
    "train_x, test_x, train_y, test_y = cross_validation.train_test_split(\n",
    "                                    x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print( \"Dimension of Boston test_x = \", test_x.shape )\n",
    "print( \"Dimension of test_y = \", test_y.shape )\n",
    "\n",
    "print( \"Dimension of Boston train_x = \", train_x.shape )\n",
    "print( \"Dimension of train_y = \", train_y.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the inputs to have mean 0 and standard variation 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler( )\n",
    "train_x = scaler.fit_transform( train_x )\n",
    "test_x  = scaler.fit_transform( test_x )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that we have 13 features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features =  13\n"
     ]
    }
   ],
   "source": [
    "numFeatures =  train_x.shape[1] \n",
    "\n",
    "print( \"number of features = \", numFeatures )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Input & Output Place-Holders</h2>\n",
    "<br />\n",
    "Define 2 place holders to the graph, one for the inputs one for the outputs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"IO\"):\n",
    "    inputs = tf.placeholder(tf.float32, [None, numFeatures], name=\"X\")\n",
    "    outputs = tf.placeholder(tf.float32, [None, 1], name=\"Yhat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define the Coeffs for the Layers</h2>\n",
    "<br />\n",
    "For each layer the input vector will be multiplied by a matrix $h$ of dim $n$ x $m$, where $n$ is the dimension of the input vector and $m$ the dimention of the output vector.   Then a bias vector of dimension $m$ is added to the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"LAYER\"):\n",
    "    # network architecture\n",
    "    Layers = [numFeatures, 52, 104, 52, 52, 52, 1]\n",
    "    h = []\n",
    "    b = []\n",
    "    for i in range( 1, len( Layers ) ):\n",
    "        h.append( tf.Variable(tf.random_normal([Layers[i-1], Layers[i]], 0, 0.1, dtype=tf.float32), name=\"h%d\" % i ) )\n",
    "        b.append( tf.Variable(tf.random_normal([Layers[i]], 0, 0.1, dtype=tf.float32 ), name=\"b%d\" % i ) )\n",
    "   \n",
    "    dropout = 0.990                           # Dropout, probability to keep units\n",
    "    keep_prob = tf.placeholder(tf.float32)   # dropout (keep probability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define the Layer operations as a Python funtion</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model( inputs, h, b ):\n",
    "    lastY = inputs\n",
    "    for i, (hi, bi) in enumerate( zip( h, b ) ):\n",
    "        y =  tf.add( tf.matmul( lastY, h[i]), b[i] )    \n",
    "        \n",
    "        if i==len(h)-1:\n",
    "            return y\n",
    "        \n",
    "        lastY =  tf.nn.sigmoid( y )\n",
    "        lastY =  tf.nn.dropout( lastY, dropout )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Define the operations that are performed</h2>\n",
    "<br />\n",
    "We define what happens to the inputs (x), when they are provided, and what we do with \n",
    "the outputs of the layers (compare them to the y values), and the type of minimization \n",
    "that must be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"train\"):\n",
    "\n",
    "    learning_rate = 0.250\n",
    "    #yout = model2( inputs, [h1, b1, h2, b2, h3, b3, hout, bout] )\n",
    "    yout = model( inputs, h, b )\n",
    "    \n",
    "    cost_op = tf.reduce_mean( tf.pow( yout - outputs, 2 ))\n",
    "    #cost_op = tf.reduce_sum( tf.pow( yout - outputs, 2 ))\n",
    "    #cost_op =  tf.reduce_mean(-tf.reduce_sum( yout * tf.log( outputs ) ) )\n",
    "\n",
    "    #train_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_op)\n",
    "    #train_op = tf.train.AdamOptimizer( learning_rate=learning_rate ).minimize( cost_op )\n",
    "    train_op = tf.train.AdagradOptimizer( learning_rate=learning_rate ).minimize( cost_op )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Train the Model</h2>\n",
    "<br />\n",
    "We are now ready to go through many sessions, and in each one train the model.  Here we train on the whole x-train and y-train data, rather than batching into smaller groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size =  50\n",
      "test length=  404\n",
      "number batches =  8\n",
      "--- Beginning Training ---\n",
      "Epoch: 0 - Error diff: 118.00605869\n"
     ]
    }
   ],
   "source": [
    "# define variables/constants that control the training\n",
    "epoch       = 0          # counter for number of rounds training network\n",
    "last_cost   = 0          # keep track of last cost to measure difference\n",
    "max_epochs  = 20000      # total number of training sessions\n",
    "tolerance   = 1e-6       # we stop when diff in costs less than that\n",
    "batch_size  = 50         # we batch the data in groups of this size\n",
    "num_samples = train_y.shape[0]                  # number of samples in training set\n",
    "num_batches = int( num_samples / batch_size )   # compute number of batches, given \n",
    "                                                # batch size\n",
    "    \n",
    "\n",
    "print( \"batch size = \", batch_size )\n",
    "print( \"test length= \", num_samples )\n",
    "print( \"number batches = \", num_batches )\n",
    "print( \"--- Beginning Training ---\" )\n",
    "\n",
    "sess = tf.Session() # Create TensorFlow session\n",
    "with sess.as_default():\n",
    "    \n",
    "    # initialize the variables\n",
    "    init = tf.initialize_all_variables()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # start training until we stop, either because we've reached the max\n",
    "    # number of epochs, or successive errors are close enough to each other\n",
    "    # (less than tolerance)\n",
    "    \n",
    "    costs = []\n",
    "    epochs= []\n",
    "    while True:\n",
    "        # Do the training\n",
    "        cost = 0\n",
    "        for n in range(  num_batches ):\n",
    "            batch_x = train_x[ n*batch_size : (n+1)*batch_size ]\n",
    "            batch_y = train_y[ n*batch_size : (n+1)*batch_size ]\n",
    "            sess.run( train_op, feed_dict={inputs: batch_x, outputs: batch_y} )\n",
    "            c = sess.run(cost_op, feed_dict={inputs: batch_x, outputs: batch_y} )\n",
    "            cost += c\n",
    "        cost /= num_batches\n",
    "        \n",
    "        costs.append( cost )\n",
    "        epochs.append( epoch )\n",
    "            \n",
    "        # Update the user every 1000 epochs\n",
    "        if epoch % 1000==0:\n",
    "            print( \"Epoch: %d - Error diff: %1.8f\" %(epoch, cost) )\n",
    "            \n",
    "            # time to stop?\n",
    "            if epoch > max_epochs  or abs(last_cost - cost) < tolerance:\n",
    "                print( \"--- STOPPING ---\" )\n",
    "                break\n",
    "            last_cost = cost\n",
    "            \n",
    "        epoch += 1\n",
    "    \n",
    "    # we're done...\n",
    "    # print some statistics...\n",
    "    \n",
    "    print( \"Test Cost =\", sess.run(cost_op, feed_dict={inputs: test_x, outputs: test_y}) )\n",
    "\n",
    "    # compute the predicted output for test_x\n",
    "    pred_y = sess.run( yout, feed_dict={inputs: test_x, outputs: test_y} )\n",
    "    \n",
    "    print( \"\\nA few predictions versus real data from test set\\nPrediction\\nreal\\tpredicted\" )\n",
    "    for (y, yHat ) in zip( test_y, pred_y )[0:10]:\n",
    "        print( \"%1.1f\\t%1.1f\" % (y, yHat ) )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>R2 score</h2>\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 =  metrics.r2_score(test_y, pred_y) \n",
    "print( \"mean squared error = \", metrics.mean_squared_error(test_y, pred_y))\n",
    "print( \"r2 score (coef determination) = \", metrics.r2_score(test_y, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Plot Prediction vs. Real Housing Price</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    fig = plt.figure()\n",
    "    xmin = min(test_y) \n",
    "    xmax = max(test_y) + 5\n",
    "    plt.xlim(xmin, xmax)\n",
    "\n",
    "    x = np.linspace( xmin, xmax )\n",
    "    plt.scatter( test_y, pred_y )\n",
    "    plt.plot( x, x )\n",
    "    \n",
    "    plt.text(5, 50, r'r2 = %1.4f' % r2)\n",
    "    plt.xlabel( \"Test y\" )\n",
    "    plt.ylabel( \"predicted y\" )\n",
    "    plt.title( \"Prediction vs. Actual Y\" )\n",
    "    #plt.save( \"images/sigmoid_adagrad_52_39_26_13_1.png\")\n",
    "    plt.show()\n",
    "    fig.savefig('files/PredVsRealBoston.png', bbox_inches='tight')\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.scatter( test_y, - test_y + pred_y )\n",
    "    plt.axhline(0, color='black')\n",
    "    plt.xlabel( \"Test y\" )\n",
    "    plt.ylabel( \"Test y - Predicted Y\" )\n",
    "    plt.title( \"Residuals\" )\n",
    "    plt.show()\n",
    "    fig.savefig('files/ResidualsBoston.png', bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Plot Cost vs Epochs</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    fig = plt.figure()\n",
    "    plt.semilogy( epochs, costs )\n",
    "    plt.xlabel( \"Epochs\" )\n",
    "    plt.ylabel( \"Cost\" )\n",
    "    plt.title( \"Cost vs. Epochs\")\n",
    "    plt.show()\n",
    "    fig.savefig('files/CostVsEpochs.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
