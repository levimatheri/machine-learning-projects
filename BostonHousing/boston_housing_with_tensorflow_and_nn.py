# -*- coding: utf-8 -*-
"""Boston Housing with TensorFlow and NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ltyKOzl2QHnm5Xu1mXrXZjtVku6Q-nNF
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler

def get_data(filePath):
    return pd.read_csv(filePath)

def remove_skew(dataframe):
    for i in dataframe.columns:
        if dataframe[i].skew() >= 0.30:
            dataframe[i] = np.log1p(dataframe[i])
        elif dataframe[i].skew() <= -0.30:
            dataframe[i] = np.square(dataframe[i])
    return dataframe

def normalize(dataframe):
    scaler = MinMaxScaler()
    return pd.DataFrame(scaler.fit_transform(dataframe), columns=dataframe.columns)

def model(inputs, num_features):
    # hidden layer 1
    W_1 = tf.Variable(tf.random_normal([num_features, 8], 0, 0.1, dtype=tf.float32))
    b_1 = tf.Variable(tf.zeros([8], dtype=tf.float32))
    layer_1 = tf.add(tf.matmul(inputs, W_1), b_1)
    layer_1_result = tf.nn.sigmoid(layer_1)

    # hidden layer 2
    W_2 = tf.Variable(tf.random_normal([8, 5], 0, 0.1, dtype=tf.float32))
    b_2 = tf.Variable(tf.zeros([5], dtype=tf.float32))
    layer_2 = tf.add(tf.matmul(layer_1_result, W_2), b_2)
    layer_2_result = tf.nn.sigmoid(layer_2)
    
    # output layer
    W_o = tf.Variable(tf.random_normal([5, 1], 0, 0.1, dtype=tf.float32))
    b_o = tf.Variable(tf.zeros([1], dtype=tf.float32))
    layer_o = tf.add(tf.matmul(layer_2_result, W_o), b_o)
    layer_o_result = tf.nn.sigmoid(layer_o)

    return layer_o_result
#   lastY = inputs
#   for i, (Wi, bi) in enumerate(zip(W, b)):
#     y = tf.add(tf.matmul(lastY, W[i]), b[i])

#     if i == len(W)-1:
#       return y
#     lastY = tf.nn.sigmoid(y)

from google.colab import files
files.upload()

# obtain dataframe
df = get_data('data.csv')

# remove skewness
df = remove_skew(df)

# scale data
df = normalize(df)

from sklearn.model_selection import train_test_split

x = pd.DataFrame(data=df.drop(columns=['medv']))
y = df['medv']

# split data to train and test
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33)

y_train = y_train.values.reshape(-1, 1)
y_test = y_test.values.reshape(-1, 1)

num_features = x_train.shape[1]

inputs = tf.placeholder(tf.float32, shape=[None, num_features], name='X_in')
outputs = tf.placeholder(tf.float32, shape=[None, 1], name='Y_out')

# create variables
batch_size = 50
num_batches = int(x_train.shape[0] / batch_size)

# layers = [num_features, 5, 1]
# W = []
# b = []

# for i in range(1, len(layers)):
#   W.append(tf.Variable(tf.random_normal([layers[i-1], layers[i]], 0, 0.1, dtype=tf.float32)))
#   b.append(tf.Variable(tf.random_normal([layers[i]])))

y_hat = model(inputs, num_features)

learning_rate = 0.001

cost_op = tf.reduce_mean(tf.pow(y_hat - outputs, 2))
train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost_op)

total_epochs = 20000

# train model
sess = tf.Session()
with sess.as_default():
    # initialize vars
    sess.run(tf.global_variables_initializer())

    costs = []
    epochs = []
    cost = 0.0
    epoch = 0
    # train until epochs total
    while True:
        for n in range(num_batches):
            x_batch = x_train[n * batch_size : (n + 1) * batch_size]
            y_batch = y_train[n * batch_size : (n + 1) * batch_size]

            sess.run(train, feed_dict={inputs: x_batch, outputs: y_batch})
            c = sess.run(cost_op, feed_dict={inputs: x_batch, outputs: y_batch})
            cost += c
        cost /= num_batches
        costs.append(cost)
        epochs.append(epoch)
        epoch += 1

        if epoch % 1000 == 0:
            print("Cost after %d epochs: %1.8f" % (epoch, cost))
        if epoch >= total_epochs:
            break

print("Test cost: ", sess.run(cost_op, feed_dict={inputs: x_test, outputs: y_test}))

import matplotlib.pyplot as plt

plt.figure()
plt.title('Training cost plot')
plt.xlabel('Epochs')
plt.ylabel('Cost')
plt.plot(epochs, costs, color='r', label='Training cost')


y_pred = sess.run(y_hat, feed_dict={inputs: x_test, outputs: y_test})

print("\nPrediction\nreal\tpredicted")
for (y, y_) in list(zip(y_test, y_pred))[0:10]:
  print("%1.1f\t%1.1f" % (y, y_))

from sklearn import metrics

r2 = metrics.r2_score(y_test, y_pred)
print("MSE: ", metrics.mean_squared_error(y_test, y_pred))
print("r2 score: ", metrics.r2_score(y_test, y_pred))

